{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nourhenesarraj/MLops_tuto/blob/main/MLops_tuto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z96cw82_V_Zy",
        "outputId": "654696cf-f5a0-48fe-92f8-74929e34349d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MLOps-Basics'...\n",
            "remote: Enumerating objects: 569, done.\u001b[K\n",
            "remote: Counting objects: 100% (569/569), done.\u001b[K\n",
            "remote: Compressing objects: 100% (371/371), done.\u001b[K\n",
            "remote: Total 569 (delta 324), reused 383 (delta 163), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (569/569), 19.05 MiB | 21.00 MiB/s, done.\n",
            "Resolving deltas: 100% (324/324), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/graviraja/MLOps-Basics.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5yJBxodYnVG"
      },
      "outputs": [],
      "source": [
        "!pip install -r /content/MLOps-Basics/week_0_project_setup/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oLUaDeL8Xpvp",
        "outputId": "882f775f-ea1f-4695-ca75-ef62a632be86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: 100% 382/382 [00:00<00:00, 289kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 320kB/s] \n",
            "Downloading: 100% 17.7M/17.7M [00:00<00:00, 19.8MB/s]\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "Downloading: 28.8kB [00:00, 14.8MB/s]       \n",
            "Downloading: 28.7kB [00:00, 15.6MB/s]       \n",
            "Downloading and preparing dataset glue/cola (download: 368.14 KiB, generated: 596.73 KiB, post-processed: Unknown size, total: 964.86 KiB) to /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "Downloading: 100% 377k/377k [00:00<00:00, 479kB/s] \n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 9/9 [00:01<00:00,  7.04ba/s]\n",
            "100% 2/2 [00:00<00:00, 11.86ba/s]\n",
            "\n",
            "  | Name | Type      | Params\n",
            "-----------------------------------\n",
            "0 | bert | BertModel | 4.4 M \n",
            "1 | W    | Linear    | 258   \n",
            "-----------------------------------\n",
            "4.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "4.4 M     Total params\n",
            "17.545    Total estimated model params size (MB)\n",
            "Epoch 0:  93% 280/301 [10:14<00:46,  2.19s/it, loss=0.617, v_num=1, val_loss=0.652, val_acc=0.688, train_loss=0.527]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/33 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: 100% 300/301 [10:30<00:02,  2.10s/it, loss=0.617, v_num=1, val_loss=0.652, val_acc=0.688, train_loss=0.527]\n",
            "Epoch 0: 100% 301/301 [10:40<00:00,  2.13s/it, loss=0.622, v_num=1, val_loss=0.633, val_acc=0.691, train_loss=0.781]\n",
            "Epoch 1:  93% 280/301 [10:37<00:47,  2.28s/it, loss=0.612, v_num=1, val_loss=0.633, val_acc=0.691, train_loss=0.512]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/33 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: 100% 300/301 [10:54<00:02,  2.18s/it, loss=0.612, v_num=1, val_loss=0.633, val_acc=0.691, train_loss=0.512]\n",
            "Epoch 1: 100% 301/301 [11:04<00:00,  2.21s/it, loss=0.609, v_num=1, val_loss=0.618, val_acc=0.691, train_loss=0.497]\n",
            "Epoch 2:  93% 280/301 [10:32<00:47,  2.26s/it, loss=0.632, v_num=1, val_loss=0.618, val_acc=0.691, train_loss=0.638]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/33 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100% 300/301 [10:47<00:02,  2.16s/it, loss=0.632, v_num=1, val_loss=0.618, val_acc=0.691, train_loss=0.638]\n",
            "Epoch 2: 100% 301/301 [10:57<00:00,  2.19s/it, loss=0.644, v_num=1, val_loss=0.618, val_acc=0.691, train_loss=0.702]\n",
            "Epoch 3:  93% 280/301 [10:53<00:49,  2.33s/it, loss=0.593, v_num=1, val_loss=0.618, val_acc=0.691, train_loss=0.559]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/33 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: 100% 300/301 [11:09<00:02,  2.23s/it, loss=0.593, v_num=1, val_loss=0.618, val_acc=0.691, train_loss=0.559]\n",
            "Epoch 3: 100% 301/301 [11:19<00:00,  2.26s/it, loss=0.6, v_num=1, val_loss=0.618, val_acc=0.691, train_loss=0.491]  \n",
            "Epoch 4:  93% 280/301 [11:10<00:50,  2.40s/it, loss=0.588, v_num=1, val_loss=0.618, val_acc=0.691, train_loss=0.544]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/33 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4: 100% 300/301 [11:26<00:02,  2.29s/it, loss=0.588, v_num=1, val_loss=0.618, val_acc=0.691, train_loss=0.544]\n",
            "Epoch 4: 100% 301/301 [11:36<00:00,  2.31s/it, loss=0.609, v_num=1, val_loss=0.620, val_acc=0.691, train_loss=0.510]\n",
            "Epoch 4: 100% 301/301 [11:36<00:00,  2.31s/it, loss=0.609, v_num=1, val_loss=0.620, val_acc=0.691, train_loss=0.510]\n"
          ]
        }
      ],
      "source": [
        "!python /content/MLOps-Basics/week_0_project_setup/train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5s0xzOxiOGl"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5KdH3twi4Ge",
        "outputId": "669bf7c4-6db1-406b-9643-3fd215805870"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_oC_mJRjEuZ",
        "outputId": "6e172eb7-e919-441d-b8a5-6fe16539f75f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "GPU available: True, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "Reusing dataset glue (/root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "Parameter 'function'=<bound method DataModule.tokenize_data of <data.DataModule object at 0x7f3f708bcdd0>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "100% 9/9 [00:02<00:00,  4.40ba/s]\n",
            "100% 2/2 [00:00<00:00, 30.94ba/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnourhenesarraj\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwarm-violet-3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/nourhenesarraj/MLops%20Tuto\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/nourhenesarraj/MLops%20Tuto/runs/13mnom9n\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20220214_100759-13mnom9n\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\n",
            "  | Name                   | Type                          | Params\n",
            "-------------------------------------------------------------------------\n",
            "0 | bert                   | BertForSequenceClassification | 4.4 M \n",
            "1 | train_accuracy_metric  | Accuracy                      | 0     \n",
            "2 | val_accuracy_metric    | Accuracy                      | 0     \n",
            "3 | f1_metric              | F1                            | 0     \n",
            "4 | precision_macro_metric | Precision                     | 0     \n",
            "5 | recall_macro_metric    | Recall                        | 0     \n",
            "6 | precision_micro_metric | Precision                     | 0     \n",
            "7 | recall_micro_metric    | Recall                        | 0     \n",
            "-------------------------------------------------------------------------\n",
            "4.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "4.4 M     Total params\n",
            "17.545    Total estimated model params size (MB)\n",
            "Epoch 0:  93% 140/151 [01:27<00:06,  1.60it/s, loss=0.616, v_num=om9n, valid/loss_epoch=0.710, valid/acc=0.336, valid/precision_macro=0.406, valid/recall_macro=0.444, valid/precision_micro=0.336, valid/recall_micro=0.336, valid/f1=0.336, train/loss_step=0.587, train/acc_step=0.734]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: 100% 151/151 [01:32<00:00,  1.63it/s, loss=0.612, v_num=om9n, valid/loss_epoch=0.618, valid/acc=0.691, valid/precision_macro=0.346, valid/recall_macro=0.500, valid/precision_micro=0.691, valid/recall_micro=0.691, valid/f1=0.691, train/loss_step=0.530, train/acc_step=0.795, train/loss_epoch=0.623, train/acc_epoch=0.690, valid/loss_step=0.492]\n",
            "Epoch 1:  93% 140/151 [01:27<00:06,  1.61it/s, loss=0.595, v_num=om9n, valid/loss_epoch=0.618, valid/acc=0.691, valid/precision_macro=0.346, valid/recall_macro=0.500, valid/precision_micro=0.691, valid/recall_micro=0.691, valid/f1=0.691, train/loss_step=0.684, train/acc_step=0.625, train/loss_epoch=0.623, train/acc_epoch=0.690, valid/loss_step=0.492]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: 100% 151/151 [01:32<00:00,  1.64it/s, loss=0.592, v_num=om9n, valid/loss_epoch=0.621, valid/acc=0.691, valid/precision_macro=0.346, valid/recall_macro=0.500, valid/precision_micro=0.691, valid/recall_micro=0.691, valid/f1=0.691, train/loss_step=0.541, train/acc_step=0.769, train/loss_epoch=0.607, train/acc_epoch=0.704, valid/loss_step=0.471]\n",
            "Epoch 2:  93% 140/151 [01:26<00:06,  1.62it/s, loss=0.594, v_num=om9n, valid/loss_epoch=0.621, valid/acc=0.691, valid/precision_macro=0.346, valid/recall_macro=0.500, valid/precision_micro=0.691, valid/recall_micro=0.691, valid/f1=0.691, train/loss_step=0.624, train/acc_step=0.688, train/loss_epoch=0.607, train/acc_epoch=0.704, valid/loss_step=0.471]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100% 151/151 [01:32<00:00,  1.64it/s, loss=0.609, v_num=om9n, valid/loss_epoch=0.618, valid/acc=0.691, valid/precision_macro=0.346, valid/recall_macro=0.500, valid/precision_micro=0.691, valid/recall_micro=0.691, valid/f1=0.691, train/loss_step=0.659, train/acc_step=0.641, train/loss_epoch=0.606, train/acc_epoch=0.704, valid/loss_step=0.491]\n",
            "Epoch 2: 100% 151/151 [01:32<00:00,  1.64it/s, loss=0.609, v_num=om9n, valid/loss_epoch=0.618, valid/acc=0.691, valid/precision_macro=0.346, valid/recall_macro=0.500, valid/precision_micro=0.691, valid/recall_micro=0.691, valid/f1=0.691, train/loss_step=0.659, train/acc_step=0.641, train/loss_epoch=0.606, train/acc_epoch=0.704, valid/loss_step=0.491]\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 647... (success).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅█████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               global_step ▁▃▆█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc_epoch ▁██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/acc_step ▂▆▅▄▆▆▆▂▃▅▅▄▂▃▇▅▅▃▅▅▄▃▇█▇▆▄▄▁▆▄▁▃▄▃▄▇▅▄▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss_epoch █▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/loss_step ▆▄▄▅▃▃▃▇▅▃▄▅▆▅▂▄▄▆▃▄▅▅▂▁▂▃▅▄█▂▅▇▅▄▅▅▂▃▅▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       trainer/global_step ▁▂▂▂▃▃▁▁▁▁▁▁▁▃▄▄▅▅▆▁▁▁▁▁▁▁▆▆▇▇▇█▁▁▁▁▁▁▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 valid/acc ▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  valid/f1 ▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          valid/loss_epoch ▁█▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   valid/loss_step/epoch_0 ▆█▇▆▆▃▁█▅█▇▇▄▆▄▅▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   valid/loss_step/epoch_1 ▆█▇▆▆▃▁█▅█▇▇▄▆▄▅▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   valid/loss_step/epoch_2 ▆█▇▆▆▃▁█▅█▇▆▄▇▄▅▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     valid/precision_macro ▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     valid/precision_micro ▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/recall_macro ▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/recall_micro ▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               global_step 401\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc_epoch 0.70436\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/acc_step 0.67188\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss_epoch 0.60614\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/loss_step 0.63341\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       trainer/global_step 401\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 valid/acc 0.69128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  valid/f1 0.69128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          valid/loss_epoch 0.61801\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   valid/loss_step/epoch_0 0.49206\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   valid/loss_step/epoch_1 0.47099\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   valid/loss_step/epoch_2 0.49054\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     valid/precision_macro 0.34564\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     valid/precision_micro 0.69128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/recall_macro 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/recall_micro 0.69128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 8 media file(s), 4 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mwarm-violet-3\u001b[0m: \u001b[34mhttps://wandb.ai/nourhenesarraj/MLops%20Tuto/runs/13mnom9n\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20220214_100759-13mnom9n/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n"
          ]
        }
      ],
      "source": [
        "!python /content/MLOps-Basics/week_1_wandb_logging/train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "Wv2oFxShvjJ7",
        "outputId": "dd115080-28ac-4e9d-bb8e-f4c1de83b99b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting hydra-core==1.1.0\n",
            "  Downloading hydra_core-1.1.0-py3-none-any.whl (144 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▎                             | 10 kB 32.3 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 20 kB 21.1 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 30 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 40 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 51 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 61 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 71 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 81 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 92 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 102 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 112 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 122 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 133 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 143 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 144 kB 8.0 MB/s \n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[?25l\r\u001b[K     |███                             | 10 kB 43.1 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 20 kB 50.8 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 30 kB 59.1 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 40 kB 63.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 51 kB 59.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 61 kB 63.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 71 kB 63.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 81 kB 65.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 92 kB 67.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 102 kB 69.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112 kB 69.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core==1.1.0) (5.4.0)\n",
            "Collecting omegaconf==2.1.*\n",
            "  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from omegaconf==2.1.*->hydra-core==1.1.0) (6.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core==1.1.0) (3.7.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=522912e00799ec2a5ae953969400128c091992709692ba983ed96f64947e8d37\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, omegaconf, hydra-core\n",
            "Successfully installed antlr4-python3-runtime-4.8 hydra-core-1.1.0 omegaconf-2.1.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install hydra-core==1.1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6s5LNzEH_pta"
      },
      "outputs": [],
      "source": [
        "!pip install -r /content/MLOps-Basics/week_2_hydra_config/requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHxHHq0dCtMD",
        "outputId": "4c3d3a41-2296-4155-95e9-e62e1683ad3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\u001b[36m2022-02-14 11:45:36,829\u001b[0m][\u001b[35mHYDRA\u001b[0m] Launching 2 jobs locally\u001b[0m\n",
            "[\u001b[36m2022-02-14 11:45:36,829\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#0 : training.max_epochs=1\u001b[0m\n",
            "[\u001b[36m2022-02-14 11:45:36,962\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - model:\n",
            "  name: google/bert_uncased_L-2_H-128_A-2\n",
            "  tokenizer: google/bert_uncased_L-2_H-128_A-2\n",
            "processing:\n",
            "  batch_size: 64\n",
            "  max_length: 128\n",
            "training:\n",
            "  max_epochs: 1\n",
            "  log_every_n_steps: 10\n",
            "  deterministic: true\n",
            "  limit_train_batches: 0.25\n",
            "  limit_val_batches: 0.25\n",
            "\u001b[0m\n",
            "[\u001b[36m2022-02-14 11:45:36,963\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - Using the model: google/bert_uncased_L-2_H-128_A-2\u001b[0m\n",
            "[\u001b[36m2022-02-14 11:45:36,963\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - Using the tokenizer: google/bert_uncased_L-2_H-128_A-2\u001b[0m\n",
            "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "GPU available: True, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "[\u001b[36m2022-02-14 11:45:40,016\u001b[0m][\u001b[34mdatasets.builder\u001b[0m][\u001b[33mWARNING\u001b[0m] - Reusing dataset glue (/root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\u001b[0m\n",
            "[\u001b[36m2022-02-14 11:45:40,028\u001b[0m][\u001b[34mdatasets.fingerprint\u001b[0m][\u001b[33mWARNING\u001b[0m] - Parameter 'function'=<bound method DataModule.tokenize_data of <data.DataModule object at 0x7f7463889cd0>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\u001b[0m\n",
            "100% 9/9 [00:02<00:00,  4.28ba/s]\n",
            "100% 2/2 [00:00<00:00, 33.86ba/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnourhenesarraj\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mheartfelt-romance-5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/nourhenesarraj/MLops%20Tuto\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/nourhenesarraj/MLops%20Tuto/runs/h8s27ow2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/multirun/2022-02-14/11-45-36/0/wandb/run-20220214_114542-h8s27ow2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\n",
            "  | Name                   | Type                          | Params\n",
            "-------------------------------------------------------------------------\n",
            "0 | bert                   | BertForSequenceClassification | 4.4 M \n",
            "1 | train_accuracy_metric  | Accuracy                      | 0     \n",
            "2 | val_accuracy_metric    | Accuracy                      | 0     \n",
            "3 | f1_metric              | F1                            | 0     \n",
            "4 | precision_macro_metric | Precision                     | 0     \n",
            "5 | recall_macro_metric    | Recall                        | 0     \n",
            "6 | precision_micro_metric | Precision                     | 0     \n",
            "7 | recall_micro_metric    | Recall                        | 0     \n",
            "-------------------------------------------------------------------------\n",
            "4.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "4.4 M     Total params\n",
            "17.545    Total estimated model params size (MB)\n",
            "Epoch 0: 100% 37/37 [00:21<00:00,  1.69it/s, loss=0.644, v_num=7ow2, valid/loss_epoch=0.666, valid/acc=0.656, valid/precision_macro=0.577, valid/recall_macro=0.512, valid/precision_micro=0.656, valid/recall_micro=0.656, valid/f1=0.656, train/loss_step=0.639, train/acc_step=0.688]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: 100% 37/37 [00:23<00:00,  1.55it/s, loss=0.608, v_num=7ow2, valid/loss_epoch=0.644, valid/acc=0.656, valid/precision_macro=0.328, valid/recall_macro=0.500, valid/precision_micro=0.656, valid/recall_micro=0.656, valid/f1=0.656, train/loss_step=0.555, train/acc_step=0.797, train/loss_epoch=0.626, train/acc_epoch=0.703, valid/loss_step=0.630]\n",
            "Epoch 0: 100% 37/37 [00:23<00:00,  1.54it/s, loss=0.608, v_num=7ow2, valid/loss_epoch=0.644, valid/acc=0.656, valid/precision_macro=0.328, valid/recall_macro=0.500, valid/precision_micro=0.656, valid/recall_micro=0.656, valid/f1=0.656, train/loss_step=0.555, train/acc_step=0.797, train/loss_epoch=0.626, train/acc_epoch=0.703, valid/loss_step=0.630]\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1413... (success).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch ▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               global_step ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc_epoch ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/acc_step ▃▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss_epoch ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/loss_step ██▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       trainer/global_step ▃▅▇█▁▁▁▂▂▂█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 valid/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  valid/f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          valid/loss_epoch ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   valid/loss_step/epoch_0 ▁█▅▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     valid/precision_macro ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     valid/precision_micro ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/recall_macro ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/recall_micro ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               global_step 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc_epoch 0.70265\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/acc_step 0.76562\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss_epoch 0.62559\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/loss_step 0.58243\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       trainer/global_step 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 valid/acc 0.65625\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  valid/f1 0.65625\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          valid/loss_epoch 0.64429\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   valid/loss_step/epoch_0 0.62965\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     valid/precision_macro 0.32812\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     valid/precision_micro 0.65625\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/recall_macro 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/recall_micro 0.65625\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 4 media file(s), 4 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mheartfelt-romance-5\u001b[0m: \u001b[34mhttps://wandb.ai/nourhenesarraj/MLops%20Tuto/runs/h8s27ow2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20220214_114542-h8s27ow2/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "[\u001b[36m2022-02-14 11:46:16,853\u001b[0m][\u001b[35mHYDRA\u001b[0m] \t#1 : training.max_epochs=2\u001b[0m\n",
            "[\u001b[36m2022-02-14 11:46:16,997\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - model:\n",
            "  name: google/bert_uncased_L-2_H-128_A-2\n",
            "  tokenizer: google/bert_uncased_L-2_H-128_A-2\n",
            "processing:\n",
            "  batch_size: 64\n",
            "  max_length: 128\n",
            "training:\n",
            "  max_epochs: 2\n",
            "  log_every_n_steps: 10\n",
            "  deterministic: true\n",
            "  limit_train_batches: 0.25\n",
            "  limit_val_batches: 0.25\n",
            "\u001b[0m\n",
            "[\u001b[36m2022-02-14 11:46:16,997\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - Using the model: google/bert_uncased_L-2_H-128_A-2\u001b[0m\n",
            "[\u001b[36m2022-02-14 11:46:16,998\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - Using the tokenizer: google/bert_uncased_L-2_H-128_A-2\u001b[0m\n",
            "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "GPU available: True, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "[\u001b[36m2022-02-14 11:46:19,695\u001b[0m][\u001b[34mdatasets.builder\u001b[0m][\u001b[33mWARNING\u001b[0m] - Reusing dataset glue (/root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\u001b[0m\n",
            "100% 9/9 [00:00<00:00, 14.88ba/s]\n",
            "100% 2/2 [00:00<00:00, 30.59ba/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mkind-candy-6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/nourhenesarraj/MLops%20Tuto\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/nourhenesarraj/MLops%20Tuto/runs/3ue7p04w\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/multirun/2022-02-14/11-45-36/1/wandb/run-20220214_114620-3ue7p04w\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\n",
            "  | Name                   | Type                          | Params\n",
            "-------------------------------------------------------------------------\n",
            "0 | bert                   | BertForSequenceClassification | 4.4 M \n",
            "1 | train_accuracy_metric  | Accuracy                      | 0     \n",
            "2 | val_accuracy_metric    | Accuracy                      | 0     \n",
            "3 | f1_metric              | F1                            | 0     \n",
            "4 | precision_macro_metric | Precision                     | 0     \n",
            "5 | recall_macro_metric    | Recall                        | 0     \n",
            "6 | precision_micro_metric | Precision                     | 0     \n",
            "7 | recall_micro_metric    | Recall                        | 0     \n",
            "-------------------------------------------------------------------------\n",
            "4.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "4.4 M     Total params\n",
            "17.545    Total estimated model params size (MB)\n",
            "Epoch 0: 100% 37/37 [00:21<00:00,  1.72it/s, loss=0.729, v_num=p04w, valid/loss_epoch=0.795, valid/acc=0.352, valid/precision_macro=0.176, valid/recall_macro=0.500, valid/precision_micro=0.352, valid/recall_micro=0.352, valid/f1=0.352, train/loss_step=0.663, train/acc_step=0.609]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: 100% 37/37 [00:23<00:00,  1.57it/s, loss=0.659, v_num=p04w, valid/loss_epoch=0.645, valid/acc=0.656, valid/precision_macro=0.328, valid/recall_macro=0.500, valid/precision_micro=0.656, valid/recall_micro=0.656, valid/f1=0.656, train/loss_step=0.608, train/acc_step=0.734, train/loss_epoch=0.698, train/acc_epoch=0.529, valid/loss_step=0.641]\n",
            "Epoch 1: 100% 37/37 [00:21<00:00,  1.70it/s, loss=0.613, v_num=p04w, valid/loss_epoch=0.645, valid/acc=0.656, valid/precision_macro=0.328, valid/recall_macro=0.500, valid/precision_micro=0.656, valid/recall_micro=0.656, valid/f1=0.656, train/loss_step=0.547, train/acc_step=0.797, train/loss_epoch=0.698, train/acc_epoch=0.529, valid/loss_step=0.641]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: 100% 37/37 [00:23<00:00,  1.56it/s, loss=0.589, v_num=p04w, valid/loss_epoch=0.650, valid/acc=0.656, valid/precision_macro=0.328, valid/recall_macro=0.500, valid/precision_micro=0.656, valid/recall_micro=0.656, valid/f1=0.656, train/loss_step=0.617, train/acc_step=0.703, train/loss_epoch=0.603, train/acc_epoch=0.721, valid/loss_step=0.636]\n",
            "Epoch 1: 100% 37/37 [00:23<00:00,  1.55it/s, loss=0.589, v_num=p04w, valid/loss_epoch=0.650, valid/acc=0.656, valid/precision_macro=0.328, valid/recall_macro=0.500, valid/precision_micro=0.656, valid/recall_micro=0.656, valid/f1=0.656, train/loss_step=0.617, train/acc_step=0.703, train/loss_epoch=0.603, train/acc_epoch=0.721, valid/loss_step=0.636]\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1481... (success).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch ▁▁▁▁▁█████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               global_step ▁▄█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc_epoch ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/acc_step ▁████▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss_epoch █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/loss_step █▄▃▂▁▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       trainer/global_step ▂▃▄▄▁▁▁▁▁▁▄▅▆▇█▁▁▁▁▁▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 valid/acc ▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  valid/f1 ▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          valid/loss_epoch ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   valid/loss_step/epoch_0 ▁█▅▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   valid/loss_step/epoch_1 ▁█▄▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     valid/precision_macro ▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     valid/precision_micro ▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/recall_macro ▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/recall_micro ▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               global_step 65\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc_epoch 0.72064\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/acc_step 0.625\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss_epoch 0.60288\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/loss_step 0.6662\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       trainer/global_step 65\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 valid/acc 0.65625\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  valid/f1 0.65625\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          valid/loss_epoch 0.65003\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   valid/loss_step/epoch_0 0.64096\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   valid/loss_step/epoch_1 0.63575\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     valid/precision_macro 0.32812\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     valid/precision_micro 0.65625\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/recall_macro 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/recall_micro 0.65625\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 6 media file(s), 4 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mkind-candy-6\u001b[0m: \u001b[34mhttps://wandb.ai/nourhenesarraj/MLops%20Tuto/runs/3ue7p04w\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20220214_114620-3ue7p04w/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n"
          ]
        }
      ],
      "source": [
        "!python /content/MLOps-Basics/week_2_hydra_config/train.py -m training.max_epochs=1,2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTTQbk-bZ3qi"
      },
      "outputs": [],
      "source": [
        "!pip install dvc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpdgmu3dj1nF",
        "outputId": "436d8687-8a10-4ed7-f406-f8626dc41320"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\u001b[36m2022-02-14 11:32:40,069\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - model:\n",
            "  name: google/bert_uncased_L-2_H-128_A-2\n",
            "  tokenizer: google/bert_uncased_L-2_H-128_A-2\n",
            "processing:\n",
            "  batch_size: 64\n",
            "  max_length: 128\n",
            "training:\n",
            "  max_epochs: 1\n",
            "  log_every_n_steps: 10\n",
            "  deterministic: true\n",
            "  limit_train_batches: 0.25\n",
            "  limit_val_batches: 0.25\n",
            "\u001b[0m\n",
            "[\u001b[36m2022-02-14 11:32:40,070\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - Using the model: google/bert_uncased_L-2_H-128_A-2\u001b[0m\n",
            "[\u001b[36m2022-02-14 11:32:40,070\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - Using the tokenizer: google/bert_uncased_L-2_H-128_A-2\u001b[0m\n",
            "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "GPU available: True, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "[\u001b[36m2022-02-14 11:32:42,820\u001b[0m][\u001b[34mdatasets.builder\u001b[0m][\u001b[33mWARNING\u001b[0m] - Reusing dataset glue (/root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\u001b[0m\n",
            "[\u001b[36m2022-02-14 11:32:42,831\u001b[0m][\u001b[34mdatasets.fingerprint\u001b[0m][\u001b[33mWARNING\u001b[0m] - Parameter 'function'=<bound method DataModule.tokenize_data of <data.DataModule object at 0x7f5e05750a50>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\u001b[0m\n",
            "100% 9/9 [00:02<00:00,  4.40ba/s]\n",
            "100% 2/2 [00:00<00:00,  9.11ba/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnourhenesarraj\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mblazing-chocolate-4\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/nourhenesarraj/MLops%20Tuto\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/nourhenesarraj/MLops%20Tuto/runs/e1948yvw\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/outputs/2022-02-14/11-32-39/wandb/run-20220214_113245-e1948yvw\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\n",
            "  | Name                   | Type                          | Params\n",
            "-------------------------------------------------------------------------\n",
            "0 | bert                   | BertForSequenceClassification | 4.4 M \n",
            "1 | train_accuracy_metric  | Accuracy                      | 0     \n",
            "2 | val_accuracy_metric    | Accuracy                      | 0     \n",
            "3 | f1_metric              | F1                            | 0     \n",
            "4 | precision_macro_metric | Precision                     | 0     \n",
            "5 | recall_macro_metric    | Recall                        | 0     \n",
            "6 | precision_micro_metric | Precision                     | 0     \n",
            "7 | recall_micro_metric    | Recall                        | 0     \n",
            "-------------------------------------------------------------------------\n",
            "4.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "4.4 M     Total params\n",
            "17.545    Total estimated model params size (MB)\n",
            "Epoch 0: 100% 37/37 [00:22<00:00,  1.66it/s, loss=0.643, v_num=8yvw, valid/loss_epoch=0.663, valid/acc=0.680, valid/precision_macro=0.786, valid/recall_macro=0.546, valid/precision_micro=0.680, valid/recall_micro=0.680, valid/f1=0.680, train/loss_step=0.636, train/acc_step=0.688]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: 100% 37/37 [00:24<00:00,  1.52it/s, loss=0.62, v_num=8yvw, valid/loss_epoch=0.644, valid/acc=0.656, valid/precision_macro=0.328, valid/recall_macro=0.500, valid/precision_micro=0.656, valid/recall_micro=0.656, valid/f1=0.656, train/loss_step=0.651, train/acc_step=0.656, train/loss_epoch=0.634, train/acc_epoch=0.696, valid/loss_step=0.633]\n",
            "Epoch 0: 100% 37/37 [00:24<00:00,  1.51it/s, loss=0.62, v_num=8yvw, valid/loss_epoch=0.644, valid/acc=0.656, valid/precision_macro=0.328, valid/recall_macro=0.500, valid/precision_micro=0.656, valid/recall_micro=0.656, valid/f1=0.656, train/loss_step=0.651, train/acc_step=0.656, train/loss_epoch=0.634, train/acc_epoch=0.696, valid/loss_step=0.633]\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1205... (success).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch ▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               global_step ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc_epoch ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/acc_step ▅█▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss_epoch ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/loss_step █▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       trainer/global_step ▃▅▇█▁▁▁▂▂▂█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 valid/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  valid/f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          valid/loss_epoch ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   valid/loss_step/epoch_0 ▁█▅▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     valid/precision_macro ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     valid/precision_micro ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/recall_macro ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/recall_micro ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               global_step 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc_epoch 0.69602\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/acc_step 0.70312\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss_epoch 0.63375\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/loss_step 0.62099\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       trainer/global_step 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 valid/acc 0.65625\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  valid/f1 0.65625\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          valid/loss_epoch 0.64443\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   valid/loss_step/epoch_0 0.63286\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     valid/precision_macro 0.32812\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     valid/precision_micro 0.65625\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/recall_macro 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/recall_micro 0.65625\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 4 media file(s), 4 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mblazing-chocolate-4\u001b[0m: \u001b[34mhttps://wandb.ai/nourhenesarraj/MLops%20Tuto/runs/e1948yvw\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20220214_113245-e1948yvw/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n"
          ]
        }
      ],
      "source": [
        "!python /content/MLOps-Basics/week_2_hydra_config/train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0mBycvaaQ61"
      },
      "outputs": [],
      "source": [
        "!pip install dvc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r-8kccDaUZy",
        "outputId": "4c4aca2f-9dba-4f3f-e804-c8cc79f1d104"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized empty Git repository in /content/.git/\n"
          ]
        }
      ],
      "source": [
        "!git init "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6htzLKTbWcB",
        "outputId": "9c0f22fd-066c-48fc-d9ba-a3c54b570eaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized DVC repository.\n",
            "\n",
            "You can now commit the changes to git.\n",
            "\n",
            "\u001b[31m+---------------------------------------------------------------------+\n",
            "\u001b[0m\u001b[31m|\u001b[0m                                                                     \u001b[31m|\u001b[0m\n",
            "\u001b[31m|\u001b[0m        DVC has enabled anonymous aggregate usage analytics.         \u001b[31m|\u001b[0m\n",
            "\u001b[31m|\u001b[0m     Read the analytics documentation (and how to opt-out) here:     \u001b[31m|\u001b[0m\n",
            "\u001b[31m|\u001b[0m             <\u001b[36mhttps://dvc.org/doc/user-guide/analytics\u001b[39m>              \u001b[31m|\u001b[0m\n",
            "\u001b[31m|\u001b[0m                                                                     \u001b[31m|\u001b[0m\n",
            "\u001b[31m+---------------------------------------------------------------------+\n",
            "\u001b[0m\n",
            "\u001b[33mWhat's next?\u001b[39m\n",
            "\u001b[33m------------\u001b[39m\n",
            "- Check out the documentation: <\u001b[36mhttps://dvc.org/doc\u001b[39m>\n",
            "- Get help and share ideas: <\u001b[36mhttps://dvc.org/chat\u001b[39m>\n",
            "- Star us on GitHub: <\u001b[36mhttps://github.com/iterative/dvc\u001b[39m>\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!dvc init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9LTWPWXf4DP",
        "outputId": "3071f1ce-bf59-4d50-ccc3-4dfcf3d99bff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reinitialized existing Git repository in /content/.git/\n"
          ]
        }
      ],
      "source": [
        "!git init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUmg7w0Gf5ae",
        "outputId": "9e649183-e4a3-4a0f-dded-d6acf6699dca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized DVC repository.\n",
            "\n",
            "You can now commit the changes to git.\n",
            "\n",
            "\u001b[31m+---------------------------------------------------------------------+\n",
            "\u001b[0m\u001b[31m|\u001b[0m                                                                     \u001b[31m|\u001b[0m\n",
            "\u001b[31m|\u001b[0m        DVC has enabled anonymous aggregate usage analytics.         \u001b[31m|\u001b[0m\n",
            "\u001b[31m|\u001b[0m     Read the analytics documentation (and how to opt-out) here:     \u001b[31m|\u001b[0m\n",
            "\u001b[31m|\u001b[0m             <\u001b[36mhttps://dvc.org/doc/user-guide/analytics\u001b[39m>              \u001b[31m|\u001b[0m\n",
            "\u001b[31m|\u001b[0m                                                                     \u001b[31m|\u001b[0m\n",
            "\u001b[31m+---------------------------------------------------------------------+\n",
            "\u001b[0m\n",
            "\u001b[33mWhat's next?\u001b[39m\n",
            "\u001b[33m------------\u001b[39m\n",
            "- Check out the documentation: <\u001b[36mhttps://dvc.org/doc\u001b[39m>\n",
            "- Get help and share ideas: <\u001b[36mhttps://dvc.org/chat\u001b[39m>\n",
            "- Star us on GitHub: <\u001b[36mhttps://github.com/iterative/dvc\u001b[39m>\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!dvc init -f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPzbB61kgA12",
        "outputId": "369eaf0f-f643-4d00-c25f-a3b0e4a12480"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On branch master\n",
            "\n",
            "No commits yet\n",
            "\n",
            "Changes to be committed:\n",
            "  (use \"git rm --cached <file>...\" to unstage)\n",
            "\n",
            "\t\u001b[32mnew file:   .dvc/.gitignore\u001b[m\n",
            "\t\u001b[32mnew file:   .dvc/config\u001b[m\n",
            "\t\u001b[32mnew file:   .dvc/plots/confusion.json\u001b[m\n",
            "\t\u001b[32mnew file:   .dvc/plots/confusion_normalized.json\u001b[m\n",
            "\t\u001b[32mnew file:   .dvc/plots/linear.json\u001b[m\n",
            "\t\u001b[32mnew file:   .dvc/plots/scatter.json\u001b[m\n",
            "\t\u001b[32mnew file:   .dvc/plots/simple.json\u001b[m\n",
            "\t\u001b[32mnew file:   .dvc/plots/smooth.json\u001b[m\n",
            "\t\u001b[32mnew file:   .dvcignore\u001b[m\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\n",
            "\t\u001b[31m.config/\u001b[m\n",
            "\t\u001b[31mMLOps-Basics/\u001b[m\n",
            "\t\u001b[31msample_data/\u001b[m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EL5J_iPbils",
        "outputId": "d46f67b3-3bf0-4834-9d56-66613c961caa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting 'storage' as a default remote.\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!dvc remote add -d storage gdrive://1FB4l-KifC6-BoVWKCEv7yDAUYqySDynJ?fbclid=IwAR144zJ0PnGFvlFifCk2xkpF7lSApiIVgdSuiKtgN_4N1BQ6uxAYGqZKkq4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMOvy-AzgJXg",
        "outputId": "7e9b9147-af18-4b8c-f2c7-70fd3703e9e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On branch master\n",
            "\n",
            "No commits yet\n",
            "\n",
            "Changes to be committed:\n",
            "  (use \"git rm --cached <file>...\" to unstage)\n",
            "\n",
            "\t\u001b[32mnew file:   .dvc/.gitignore\u001b[m\n",
            "\t\u001b[32mnew file:   .dvc/config\u001b[m\n",
            "\t\u001b[32mnew file:   .dvc/plots/confusion.json\u001b[m\n",
            "\t\u001b[32mnew file:   .dvc/plots/confusion_normalized.json\u001b[m\n",
            "\t\u001b[32mnew file:   .dvc/plots/linear.json\u001b[m\n",
            "\t\u001b[32mnew file:   .dvc/plots/scatter.json\u001b[m\n",
            "\t\u001b[32mnew file:   .dvc/plots/simple.json\u001b[m\n",
            "\t\u001b[32mnew file:   .dvc/plots/smooth.json\u001b[m\n",
            "\t\u001b[32mnew file:   .dvcignore\u001b[m\n",
            "\n",
            "Changes not staged for commit:\n",
            "  (use \"git add <file>...\" to update what will be committed)\n",
            "  (use \"git checkout -- <file>...\" to discard changes in working directory)\n",
            "\n",
            "\t\u001b[31mmodified:   .dvc/config\u001b[m\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\n",
            "\t\u001b[31m.config/\u001b[m\n",
            "\t\u001b[31mMLOps-Basics/\u001b[m\n",
            "\t\u001b[31msample_data/\u001b[m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zO-gXtrYgf85",
        "outputId": "b1ad9ce2-8eb5-4629-f21e-2ca792c77fd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[master (root-commit) 3257c15] first commit\n",
            " 9 files changed, 515 insertions(+)\n",
            " create mode 100644 .dvc/.gitignore\n",
            " create mode 100644 .dvc/config\n",
            " create mode 100644 .dvc/plots/confusion.json\n",
            " create mode 100644 .dvc/plots/confusion_normalized.json\n",
            " create mode 100644 .dvc/plots/linear.json\n",
            " create mode 100644 .dvc/plots/scatter.json\n",
            " create mode 100644 .dvc/plots/simple.json\n",
            " create mode 100644 .dvc/plots/smooth.json\n",
            " create mode 100644 .dvcignore\n"
          ]
        }
      ],
      "source": [
        "!git commit -m \"first commit\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dA2gUg_EiPVb"
      },
      "outputs": [],
      "source": [
        "!pip install -r /content/MLOps-Basics/week_2_hydra_config/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6125pmVg7Wi",
        "outputId": "de5abc14-63de-4b7a-deae-383255ff5ca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\u001b[36m2022-02-14 19:39:19,073\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - model:\n",
            "  name: google/bert_uncased_L-2_H-128_A-2\n",
            "  tokenizer: google/bert_uncased_L-2_H-128_A-2\n",
            "processing:\n",
            "  batch_size: 64\n",
            "  max_length: 128\n",
            "training:\n",
            "  max_epochs: 1\n",
            "  log_every_n_steps: 10\n",
            "  deterministic: true\n",
            "  limit_train_batches: 0.25\n",
            "  limit_val_batches: 0.25\n",
            "\u001b[0m\n",
            "[\u001b[36m2022-02-14 19:39:19,073\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - Using the model: google/bert_uncased_L-2_H-128_A-2\u001b[0m\n",
            "[\u001b[36m2022-02-14 19:39:19,074\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - Using the tokenizer: google/bert_uncased_L-2_H-128_A-2\u001b[0m\n",
            "Downloading: 100% 382/382 [00:00<00:00, 341kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 1.26MB/s]\n",
            "Downloading: 100% 17.7M/17.7M [00:00<00:00, 52.4MB/s]\n",
            "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "Downloading: 28.8kB [00:00, 18.1MB/s]       \n",
            "Downloading: 28.7kB [00:00, 22.3MB/s]       \n",
            "Downloading and preparing dataset glue/cola (download: 368.14 KiB, generated: 596.73 KiB, post-processed: Unknown size, total: 964.86 KiB) to /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "Downloading: 100% 377k/377k [00:00<00:00, 3.79MB/s]\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "[\u001b[36m2022-02-14 19:39:26,444\u001b[0m][\u001b[34mdatasets.fingerprint\u001b[0m][\u001b[33mWARNING\u001b[0m] - Parameter 'function'=<bound method DataModule.tokenize_data of <data.DataModule object at 0x7f0693de9810>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\u001b[0m\n",
            "100% 9/9 [00:00<00:00, 17.87ba/s]\n",
            "100% 2/2 [00:00<00:00, 27.23ba/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\", line 948, in init\n",
            "    wi.setup(kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\", line 193, in setup\n",
            "    _silent=settings._quiet or settings._silent,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_login.py\", line 284, in _login\n",
            "    wlogin.prompt_api_key()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_login.py\", line 212, in prompt_api_key\n",
            "    key, status = self._prompt_api_key()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_login.py\", line 196, in _prompt_api_key\n",
            "    no_create=self._settings.force if self._settings else None,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/apikey.py\", line 94, in prompt_api_key\n",
            "    choices, input_timeout=settings.login_timeout, jupyter=jupyter\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/util.py\", line 1090, in prompt_choices\n",
            "    choice = _prompt_choice(input_timeout=input_timeout, jupyter=jupyter)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/util.py\", line 1075, in _prompt_choice\n",
            "    choice = input_fn(text)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/MLOps-Basics/week_2_hydra_config/train.py\", line 82, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/hydra/main.py\", line 52, in decorated_main\n",
            "    config_name=config_name,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/hydra/_internal/utils.py\", line 378, in _run_hydra\n",
            "    lambda: hydra.run(\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/hydra/_internal/utils.py\", line 211, in run_and_report\n",
            "    return func()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/hydra/_internal/utils.py\", line 381, in <lambda>\n",
            "    overrides=args.overrides,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/hydra/_internal/hydra.py\", line 106, in run\n",
            "    configure_logging=with_log_configuration,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/hydra/core/utils.py\", line 160, in run_job\n",
            "    ret.return_value = task_function(task_cfg)\n",
            "  File \"/content/MLOps-Basics/week_2_hydra_config/train.py\", line 77, in main\n",
            "    trainer.fit(cola_model, cola_data)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 496, in fit\n",
            "    self.pre_dispatch()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 530, in pre_dispatch\n",
            "    self.logger.log_hyperparams(self.lightning_module.hparams_initial)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py\", line 48, in wrapped_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loggers/wandb.py\", line 184, in log_hyperparams\n",
            "    self.experiment.config.update(params, allow_val_change=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loggers/base.py\", line 41, in experiment\n",
            "    return get_experiment() or DummyExperiment()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py\", line 48, in wrapped_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loggers/base.py\", line 39, in get_experiment\n",
            "    return fn(self)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loggers/wandb.py\", line 163, in experiment\n",
            "    ) if wandb.run is None else wandb.run\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\", line 948, in init\n",
            "    wi.setup(kwargs)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!python /content/MLOps-Basics/week_2_hydra_config/train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0CnM9BtjV5D",
        "outputId": "971515b0-59aa-4cf7-9d23-ee6a7f492118"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K\u001b[32m⠙\u001b[0m Checking graph\n",
            "Adding...:   0% 0/1 [00:00<?, ?file/s{'info': ''}]\n",
            "!\u001b[A\n",
            "  0%|          |                                   0.00/? [00:00<?,        ?B/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Querying cache in /content/.dvc/cache:   0% 0.00/1.00 [00:00<?, ?file/s]\u001b[A\n",
            "  0% 0.00/1.00 [00:00<?, ?file/s{'info': ''}]                           \u001b[A\n",
            "                                             \u001b[A\n",
            "Transferring:   0% 0/1 [00:00<?, ?file/s]\u001b[A\n",
            "Transferring:   0% 0/1 [00:00<?, ?file/s{'info': ''}]\u001b[A\n",
            "                                                     \u001b[A\n",
            "!\u001b[A\n",
            "  0%|          |.iUEapJ8Hk9cL6GNQZCDbwj.tmp        0.00/? [00:00<?,        ?B/s]\u001b[A\n",
            ".iUEapJ8Hk9cL6GNQZCDbwj.tmp:   0% 0.00/4.00 [00:00<?, ?B/s{'info': ''}]         \u001b[A\n",
            ".iUEapJ8Hk9cL6GNQZCDbwj.tmp:   0% 0.00/4.00 [00:00<?, ?B/s{'info': ''}]\u001b[A\n",
            "                                                                       \u001b[A\n",
            "!\u001b[A\n",
            "  0%|          |88461a5ada407ca320983cdb15bdd9     0.00/? [00:00<?,        ?B/s]\u001b[A\n",
            "88461a5ada407ca320983cdb15bdd9:   0% 0.00/50.2M [00:00<?, ?B/s{'info': ''}]     \u001b[A\n",
            "88461a5ada407ca320983cdb15bdd9:   0% 0.00/50.2M [00:00<?, ?B/s{'info': ''}]\u001b[A\n",
            "Adding...: 100% 1/1 [00:00<00:00,  4.02file/s{'info': ''}]\n",
            "\n",
            "To track the changes with git, run:\n",
            "\n",
            "    git add outputs/2022-02-14/14-03-28/models/best-checkpoint.ckpt.dvc outputs/2022-02-14/14-03-28/models/.gitignore\n",
            "\n",
            "To enable auto staging, run:\n",
            "\n",
            "\tdvc config core.autostage true\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!dvc add  /content/outputs/2022-02-14/14-03-28/models/best-checkpoint.ckpt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!dvc add ../models/best-checkpoint.ckpt --file trained_model.dvc"
      ],
      "metadata": {
        "id": "VzKgFXMTvo7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0X-hOVmkaX7",
        "outputId": "d6b6996b-8e6e-4972-fa53-80cfc0918157"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r!\rIf DVC froze, see `hardlink_lock` in <\u001b[36mhttps://man.dvc.org/config#core\u001b[39m>\r                                                                      \rData and pipelines are up to date.\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!dvc status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CLGnlylj5Nr",
        "outputId": "7177ec9a-2ed8-4f8d-9cf0-c229a6e11632"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On branch master\n",
            "Changes not staged for commit:\n",
            "  (use \"git add <file>...\" to update what will be committed)\n",
            "  (use \"git checkout -- <file>...\" to discard changes in working directory)\n",
            "\n",
            "\t\u001b[31mmodified:   .dvc/config\u001b[m\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\n",
            "\t\u001b[31m.config/\u001b[m\n",
            "\t\u001b[31mMLOps-Basics/\u001b[m\n",
            "\t\u001b[31moutputs/\u001b[m\n",
            "\t\u001b[31msample_data/\u001b[m\n",
            "\n",
            "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
          ]
        }
      ],
      "source": [
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xk6VspjDk901"
      },
      "outputs": [],
      "source": [
        "!pip install dvc[gdrive]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEMBeyRVkNsZ",
        "outputId": "bf53bb34-218c-430a-cbc4-f982f5ae34b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  0% 0/1 [00:00<?, ?file/s{'info': ''}]                                         Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?client_id=710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.appdata&access_type=offline&response_type=code&approval_prompt=force\n",
            "\n",
            "Enter verification code: 4/1AX4XfWhxUS2Ekpt-gBUpn3PgznOTR1eXiuPCkYrEozbTKSmtmtAIIU_f0QM\n",
            "Authentication successful.\n",
            "Transferring:   0% 0/1 [00:00<?, ?file/s{'info': ''}]\n",
            "!\u001b[A\n",
            "  0%|          |88461a5ada407ca320983cdb15bdd9     0.00/? [00:00<?,        ?B/s]\u001b[A\n",
            "88461a5ada407ca320983cdb15bdd9:   0% 0.00/50.2M [00:00<?, ?B/s{'info': ''}]     \u001b[A\n",
            "  0% 8.00k/50.2M [00:01<2:15:38, 6.47kB/s{'info': ''}]                     \u001b[A\n",
            " 23% 11.5M/50.2M [00:01<1:13:12, 9.25kB/s{'info': ''}]\u001b[A\n",
            " 49% 24.7M/50.2M [00:01<33:50, 13.2kB/s{'info': ''}]  \u001b[A\n",
            " 72% 36.1M/50.2M [00:01<13:05, 18.9kB/s{'info': ''}]\u001b[A\n",
            " 93% 46.8M/50.2M [00:01<02:13, 27.0kB/s{'info': ''}]\u001b[A\n",
            "1 file pushed\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!dvc push /content/outputs/2022-02-14/14-03-28/models/best-checkpoint.ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jMGcesAljRV",
        "outputId": "de114f49-614c-4c64-a1fa-a9b9ad57c025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On branch master\n",
            "Changes not staged for commit:\n",
            "\t\u001b[31mmodified:   .dvc/config\u001b[m\n",
            "\n",
            "Untracked files:\n",
            "\t\u001b[31m.config/\u001b[m\n",
            "\t\u001b[31mMLOps-Basics/\u001b[m\n",
            "\t\u001b[31moutputs/\u001b[m\n",
            "\t\u001b[31msample_data/\u001b[m\n",
            "\n",
            "no changes added to commit\n"
          ]
        }
      ],
      "source": [
        "!git commit -m \"Added trained model to google drive using dvc\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgYjcdsQnK8R"
      },
      "outputs": [],
      "source": [
        "!git add ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVToY2UbnLJK",
        "outputId": "69c3a45b-e630-4e26-8890-2d3f97f65f42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[master c513626] Added trained model to google drive using dvc\n",
            " 42 files changed, 52023 insertions(+)\n",
            " create mode 100644 .config/.last_opt_in_prompt.yaml\n",
            " create mode 100644 .config/.last_survey_prompt.yaml\n",
            " create mode 100644 .config/.last_update_check.json\n",
            " create mode 100644 .config/active_config\n",
            " create mode 100644 .config/config_sentinel\n",
            " create mode 100644 .config/configurations/config_default\n",
            " create mode 100644 .config/gce\n",
            " create mode 100644 .config/logs/2022.02.01/14.30.57.022317.log\n",
            " create mode 100644 .config/logs/2022.02.01/14.31.16.993813.log\n",
            " create mode 100644 .config/logs/2022.02.01/14.31.33.364834.log\n",
            " create mode 100644 .config/logs/2022.02.01/14.31.40.709264.log\n",
            " create mode 100644 .config/logs/2022.02.01/14.31.57.576848.log\n",
            " create mode 100644 .config/logs/2022.02.01/14.31.58.218326.log\n",
            " create mode 160000 MLOps-Basics\n",
            " create mode 100644 outputs/2022-02-14/14-03-28/.hydra/config.yaml\n",
            " create mode 100644 outputs/2022-02-14/14-03-28/.hydra/hydra.yaml\n",
            " create mode 100644 outputs/2022-02-14/14-03-28/.hydra/overrides.yaml\n",
            " create mode 100644 outputs/2022-02-14/14-03-28/models/.gitignore\n",
            " create mode 100644 outputs/2022-02-14/14-03-28/models/best-checkpoint.ckpt.dvc\n",
            " create mode 100644 outputs/2022-02-14/14-03-28/train.log\n",
            " create mode 120000 outputs/2022-02-14/14-03-28/wandb/debug-internal.log\n",
            " create mode 120000 outputs/2022-02-14/14-03-28/wandb/debug.log\n",
            " create mode 120000 outputs/2022-02-14/14-03-28/wandb/latest-run\n",
            " create mode 100644 outputs/2022-02-14/14-03-28/wandb/run-20220214_140429-3gwfk2x2/files/config.yaml\n",
            " create mode 100644 outputs/2022-02-14/14-03-28/wandb/run-20220214_140429-3gwfk2x2/files/media/table/conf_table_0_e1d532a13ba5de5b9c62.table.json\n",
            " create mode 100644 outputs/2022-02-14/14-03-28/wandb/run-20220214_140429-3gwfk2x2/files/media/table/conf_table_10_53e8eba8db34a3d4f73a.table.json\n",
            " create mode 100644 outputs/2022-02-14/14-03-28/wandb/run-20220214_140429-3gwfk2x2/files/media/table/examples_11_91f6e347e7ecd02507aa.table.json\n",
            " create mode 100644 outputs/2022-02-14/14-03-28/wandb/run-20220214_140429-3gwfk2x2/files/media/table/examples_1_d4b27fb56b403e73b164.table.json\n",
            " create mode 100644 outputs/2022-02-14/14-03-28/wandb/run-20220214_140429-3gwfk2x2/files/output.log\n",
            " create mode 100644 outputs/2022-02-14/14-03-28/wandb/run-20220214_140429-3gwfk2x2/files/requirements.txt\n",
            " create mode 100644 outputs/2022-02-14/14-03-28/wandb/run-20220214_140429-3gwfk2x2/files/wandb-metadata.json\n",
            " create mode 100644 outputs/2022-02-14/14-03-28/wandb/run-20220214_140429-3gwfk2x2/files/wandb-summary.json\n",
            " create mode 100644 outputs/2022-02-14/14-03-28/wandb/run-20220214_140429-3gwfk2x2/logs/debug-internal.log\n",
            " create mode 100644 outputs/2022-02-14/14-03-28/wandb/run-20220214_140429-3gwfk2x2/logs/debug.log\n",
            " create mode 100644 outputs/2022-02-14/14-03-28/wandb/run-20220214_140429-3gwfk2x2/run-3gwfk2x2.wandb\n",
            " create mode 100755 sample_data/README.md\n",
            " create mode 100755 sample_data/anscombe.json\n",
            " create mode 100644 sample_data/california_housing_test.csv\n",
            " create mode 100644 sample_data/california_housing_train.csv\n",
            " create mode 100644 sample_data/mnist_test.csv\n",
            " create mode 100644 sample_data/mnist_train_small.csv\n"
          ]
        }
      ],
      "source": [
        "!git commit -m \"Added trained model to google drive using dvc\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ny8Sc1clnRoX"
      },
      "outputs": [],
      "source": [
        "!git branch -M main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeKEs2N_no0w"
      },
      "outputs": [],
      "source": [
        "!git remote add origin https://github.com/nourhenesarraj/MLops_tuto.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3QfD64DnvL3",
        "outputId": "d60918f5-00e9-4987-842e-95ba5b4bdd50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* \u001b[32mmain\u001b[m\n"
          ]
        }
      ],
      "source": [
        "!git branch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDyHl--noWbx",
        "outputId": "336b3707-21b2-4080-e5ca-be7189e10f4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ],
      "source": [
        "!git push -u origin main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jwq8wTfRFQf1"
      },
      "outputs": [],
      "source": [
        "!pip install -r /content/MLOps-Basics/week_3_dvc/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "AexADw2eDQrt",
        "outputId": "c19b66e3-079e-49d2-8f7a-668ce5d80c7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\u001b[36m2022-02-14 16:34:41,701\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - model:\n",
            "  name: google/bert_uncased_L-2_H-128_A-2\n",
            "  tokenizer: google/bert_uncased_L-2_H-128_A-2\n",
            "processing:\n",
            "  batch_size: 64\n",
            "  max_length: 128\n",
            "training:\n",
            "  max_epochs: 1\n",
            "  log_every_n_steps: 10\n",
            "  deterministic: true\n",
            "  limit_train_batches: 0.25\n",
            "  limit_val_batches: 0.25\n",
            "\u001b[0m\n",
            "[\u001b[36m2022-02-14 16:34:41,701\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - Using the model: google/bert_uncased_L-2_H-128_A-2\u001b[0m\n",
            "[\u001b[36m2022-02-14 16:34:41,702\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - Using the tokenizer: google/bert_uncased_L-2_H-128_A-2\u001b[0m\n",
            "Downloading: 100% 382/382 [00:00<00:00, 181kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 1.23MB/s]\n",
            "Downloading: 100% 17.7M/17.7M [00:00<00:00, 53.4MB/s]\n",
            "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "Downloading: 28.8kB [00:00, 11.3MB/s]       \n",
            "Downloading: 28.7kB [00:00, 12.1MB/s]       \n",
            "Downloading and preparing dataset glue/cola (download: 368.14 KiB, generated: 596.73 KiB, post-processed: Unknown size, total: 964.86 KiB) to /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "Downloading: 100% 377k/377k [00:00<00:00, 12.9MB/s]\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "[\u001b[36m2022-02-14 16:34:49,899\u001b[0m][\u001b[34mdatasets.fingerprint\u001b[0m][\u001b[33mWARNING\u001b[0m] - Parameter 'function'=<bound method DataModule.tokenize_data of <data.DataModule object at 0x7f575e08b2d0>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\u001b[0m\n",
            "100% 9/9 [00:00<00:00, 15.42ba/s]\n",
            "  0% 0/2 [00:00<?, ?ba/s]"
          ]
        }
      ],
      "source": [
        "!python /content/MLOps-Basics/week_3_dvc/train.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/MLOps-Basics/week_4_onnx/requirements.txt"
      ],
      "metadata": {
        "id": "Bv9FKn_JXmFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/MLOps-Basics/week_4_onnx/train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6cosg-2X3nF",
        "outputId": "f70b0638-a9c7-461f-e8ef-a15cd8e1ba32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\u001b[36m2022-02-15 07:54:40,067\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - model:\n",
            "  name: google/bert_uncased_L-2_H-128_A-2\n",
            "  tokenizer: google/bert_uncased_L-2_H-128_A-2\n",
            "processing:\n",
            "  batch_size: 64\n",
            "  max_length: 128\n",
            "training:\n",
            "  max_epochs: 1\n",
            "  log_every_n_steps: 10\n",
            "  deterministic: true\n",
            "  limit_train_batches: 0.25\n",
            "  limit_val_batches: 0.25\n",
            "\u001b[0m\n",
            "[\u001b[36m2022-02-15 07:54:40,068\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - Using the model: google/bert_uncased_L-2_H-128_A-2\u001b[0m\n",
            "[\u001b[36m2022-02-15 07:54:40,068\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - Using the tokenizer: google/bert_uncased_L-2_H-128_A-2\u001b[0m\n",
            "Downloading: 100% 382/382 [00:00<00:00, 307kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 3.90MB/s]\n",
            "Downloading: 100% 17.7M/17.7M [00:00<00:00, 45.3MB/s]\n",
            "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "Downloading: 28.8kB [00:00, 10.8MB/s]       \n",
            "Downloading: 28.7kB [00:00, 15.8MB/s]       \n",
            "Downloading and preparing dataset glue/cola (download: 368.14 KiB, generated: 596.73 KiB, post-processed: Unknown size, total: 964.86 KiB) to /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "Downloading: 100% 377k/377k [00:00<00:00, 1.06MB/s]\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "[\u001b[36m2022-02-15 07:54:46,644\u001b[0m][\u001b[34mdatasets.fingerprint\u001b[0m][\u001b[33mWARNING\u001b[0m] - Parameter 'function'=<bound method DataModule.tokenize_data of <data.DataModule object at 0x7f840beccf10>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\u001b[0m\n",
            "100% 9/9 [00:00<00:00, 14.06ba/s]\n",
            "100% 2/2 [00:00<00:00, 25.70ba/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Don't visualize my results'\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id 3a8l7umz.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to `offline` in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n",
            "\n",
            "  | Name                   | Type                          | Params\n",
            "-------------------------------------------------------------------------\n",
            "0 | bert                   | BertForSequenceClassification | 4.4 M \n",
            "1 | train_accuracy_metric  | Accuracy                      | 0     \n",
            "2 | val_accuracy_metric    | Accuracy                      | 0     \n",
            "3 | f1_metric              | F1                            | 0     \n",
            "4 | precision_macro_metric | Precision                     | 0     \n",
            "5 | recall_macro_metric    | Recall                        | 0     \n",
            "6 | precision_micro_metric | Precision                     | 0     \n",
            "7 | recall_micro_metric    | Recall                        | 0     \n",
            "-------------------------------------------------------------------------\n",
            "4.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "4.4 M     Total params\n",
            "17.545    Total estimated model params size (MB)\n",
            "Epoch 0:  93% 140/151 [01:57<00:09,  1.20it/s, loss=0.596, v_num=7umz, valid/loss_epoch=0.668, valid/acc=0.641, valid/precision_macro=0.407, valid/recall_macro=0.497, valid/precision_micro=0.641, valid/recall_micro=0.641, valid/f1=0.641, train/loss_step=0.601, train/acc_step=0.703]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: 100% 151/151 [02:03<00:00,  1.22it/s, loss=0.606, v_num=7umz, valid/loss_epoch=0.617, valid/acc=0.691, valid/precision_macro=0.346, valid/recall_macro=0.500, valid/precision_micro=0.691, valid/recall_micro=0.691, valid/f1=0.691, train/loss_step=0.619, train/acc_step=0.692, train/loss_epoch=0.612, train/acc_epoch=0.704, valid/loss_step=0.484]\n",
            "Epoch 0: 100% 151/151 [02:03<00:00,  1.22it/s, loss=0.606, v_num=7umz, valid/loss_epoch=0.617, valid/acc=0.691, valid/precision_macro=0.346, valid/recall_macro=0.500, valid/precision_micro=0.691, valid/recall_micro=0.691, valid/f1=0.691, train/loss_step=0.619, train/acc_step=0.692, train/loss_epoch=0.612, train/acc_epoch=0.704, valid/loss_step=0.484]\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 272... (success).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               global_step ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc_epoch ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/acc_step ▆█▁▅▅▆▆█▇▃▇▃▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss_epoch ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/loss_step ▅▂█▄▄▄▄▁▂▆▂▆▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       trainer/global_step ▁▂▃▃▄▄▅▅▆▆▇▇██▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 valid/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  valid/f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          valid/loss_epoch ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   valid/loss_step/epoch_0 ▆█▇▆▆▃▁█▅█▇▇▄▆▄▅▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     valid/precision_macro ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     valid/precision_micro ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/recall_macro ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/recall_micro ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               global_step 133\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/acc_epoch 0.70378\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/acc_step 0.6875\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss_epoch 0.61214\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/loss_step 0.61885\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       trainer/global_step 133\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 valid/acc 0.69128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  valid/f1 0.69128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          valid/loss_epoch 0.61714\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   valid/loss_step/epoch_0 0.48442\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     valid/precision_macro 0.34564\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     valid/precision_micro 0.69128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/recall_macro 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/recall_micro 0.69128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mwandb sync /content/outputs/2022-02-15/07-54-39/wandb/offline-run-20220215_075650-3a8l7umz\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[0mFind logs at: ./wandb/offline-run-20220215_075650-3a8l7umz/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/MLOps-Basics/week_4_onnx/convert_model_to_onnx.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0m7_1hIZZeLs",
        "outputId": "3892f7fb-e8a1-4525-96ce-f7e6b295e1f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\u001b[36m2022-02-15 08:01:02,112\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - Loading pre-trained model from: /content/models/best-checkpoint.ckpt\u001b[0m\n",
            "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[\u001b[36m2022-02-15 08:01:04,322\u001b[0m][\u001b[34mdatasets.builder\u001b[0m][\u001b[33mWARNING\u001b[0m] - Reusing dataset glue (/root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\u001b[0m\n",
            "100% 9/9 [00:02<00:00,  3.12ba/s]\n",
            "100% 2/2 [00:00<00:00, 22.70ba/s]\n",
            "[\u001b[36m2022-02-15 08:01:07,527\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - Converting the model into ONNX format\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py:1790: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  input_tensor.shape[chunk_dim] == tensor_shape for input_tensor in input_tensors\n",
            "[\u001b[36m2022-02-15 08:01:08,179\u001b[0m][\u001b[34m__main__\u001b[0m][\u001b[32mINFO\u001b[0m] - Model converted successfully. ONNX format model is at: /content/models/model.onnx\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime"
      ],
      "metadata": {
        "id": "mjBsypIjcR1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from onnxruntime import  get_all_providers\n",
        "print(get_all_providers())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA-KvZlDco8C",
        "outputId": "04944884-0f53-4bb8-964a-53ba9a15df54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'MIGraphXExecutionProvider', 'ROCMExecutionProvider', 'OpenVINOExecutionProvider', 'DnnlExecutionProvider', 'NupharExecutionProvider', 'VitisAIExecutionProvider', 'NnapiExecutionProvider', 'CoreMLExecutionProvider', 'ArmNNExecutionProvider', 'ACLExecutionProvider', 'DmlExecutionProvider', 'RknpuExecutionProvider', 'CPUExecutionProvider']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/MLOps-Basics/week_4_onnx/inference_onnx.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xKrDuoScreS",
        "outputId": "703f1dbf-6323-4912-8bc2-2d80fee45354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "function:'predict' took: 0.00478 sec\n",
            "[{'label': 'unacceptable', 'score': 0.28792667}, {'label': 'acceptable', 'score': 0.71207327}]\n",
            "function:'predict' took: 0.00362 sec\n",
            "function:'predict' took: 0.00355 sec\n",
            "function:'predict' took: 0.00361 sec\n",
            "function:'predict' took: 0.00351 sec\n",
            "function:'predict' took: 0.00410 sec\n",
            "function:'predict' took: 0.00362 sec\n",
            "function:'predict' took: 0.00362 sec\n",
            "function:'predict' took: 0.00498 sec\n",
            "function:'predict' took: 0.00364 sec\n",
            "function:'predict' took: 0.00364 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/MLOps-Basics/week_4_onnx/inference.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOvxpbLtcypg",
        "outputId": "d75e6f93-5ddc-4b86-e1e2-9d9923b04033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "function:'predict' took: 0.01018 sec\n",
            "[{'label': 'unacceptable', 'score': 0.28792667388916016}, {'label': 'acceptable', 'score': 0.7120733261108398}]\n",
            "function:'predict' took: 0.00591 sec\n",
            "function:'predict' took: 0.00589 sec\n",
            "function:'predict' took: 0.00568 sec\n",
            "function:'predict' took: 0.00568 sec\n",
            "function:'predict' took: 0.00688 sec\n",
            "function:'predict' took: 0.00580 sec\n",
            "function:'predict' took: 0.00569 sec\n",
            "function:'predict' took: 0.00606 sec\n",
            "function:'predict' took: 0.00609 sec\n",
            "function:'predict' took: 0.00590 sec\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "MLops_tuto.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPTpKZQW7Qu+iHq/BnXE+EK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}